# PingCAP 平凯星辰一面面试问答（按时间顺序）

## 目录

- [1. 项目经历展示](#1-项目经历展示)
  - [1.1 项目介绍](#11-项目介绍)
  - [1.2 Code Agent 使用](#12-code-agent-使用)
  - [1.3 模型选择](#13-模型选择)
  - [1.4 Prompt 工程](#14-prompt-工程)
  - [1.5 框架选择](#15-框架选择)
- [2. 实习经历相关](#2-实习经历相关)
  - [2.1 实习项目介绍](#21-实习项目介绍)
  - [2.2 项目优化方案](#22-项目优化方案)

---

## 1. 项目经历展示

### 1.1 项目介绍

**Q：你的项目经历，哪些方面能够展示你的优点？**

**A：** 最近打了一个 Hackathon 项目（鱼克松），跟美国同学做的改简历项目。主要用 LangGraph 实现多个 agent 协同：用户输入岗位 JD 和简历，通过多个 agent 协同改简历，渲染成 PDF 返回。因为北美找工作看重关键词匹配，所以做了这个项目。

**Q：拿奖了没？**

**A：** 拿了一个优胜奖，还有硅基流动因为我们用了他们的 API，给了 2000 块钱的奖励。

**Q：你是做前后端都会吗？**

**A：** 不是，我是做后端的，朋友做前端。我主要负责后端部分。

### 1.2 Code Agent 使用

**Q：你们这个项目里面代码有多少是 code agent 生成的？**

**A：** 后端大概有一半。我先说我要做什么，让 Cursor 帮我起一个框架，然后我再根据框架自己进行修改。比如一开始它用的工具不太对，做 agent 需要结构化输出，要保证格式符合要求，它一开始没用 pydantic，后来我改成用 pydantic。另外还做了一个 agent 来对比两个简历，总结改了什么给用户。PDF 解析的细节是我自己写的。

**Q：你用 cursor 过程中有没有遇到实际问题？除了你刚才提过的细节问题之外。**

**A：** 主要遇到几类问题：

1. **上下文与大型项目的问题**：
   - **局部视角局限**：在 monorepo 或多模块项目中，Cursor 往往只看局部文件就开始改，忽略其他模块或配置。比如项目里已经有某个工具函数，它却"重新发明一遍"
   - **上下文窗口不够全**：虽然感觉"项目都打开了，它应该知道"，但实际上它只看了少数被高亮或最近改过的文件。重构跨多个文件时，有些地方会漏改，导致编译或运行时才暴露问题
   - **本质问题**：Cursor + LLM 没法像人那样真正"整仓库全盘加载后再推理"，只是一个有限窗口的"聪明文本编辑器"

2. **代码质量 & 可控性问题**：
   - **一改就改太多**：给个模糊任务"帮我优化下这个函数"，结果函数重写了、接口签名顺带改了、调用方也"顺手修了"，review diff 的压力直线上升
   - **代码风格不一致**：项目有特定的命名习惯、错误处理方式、日志规范，但它有时候按自己的"通用习惯"写，导致同一项目里风格混杂
   - **不太管测试 & 边界**：会生成看起来很漂亮的代码，但没有跑测试的意识（除非明确说：写测试 + 更新现有 test）。边界条件、极小概率分支经常要自己补
   - **结果**："AI 写得很快，人类 review 更累"，如果不给它约束，就会有这种感觉

3. **数据验证和边界条件处理**：
   - Cursor 生成的代码往往假设理想情况，比如简历解析时，它假设所有字段都存在，但实际简历格式多样，有些字段可能缺失
   - 使用 pydantic 做结构化输出时，它默认所有字段都是 required，但实际需要很多字段设为 Optional，允许 None 值
   - 需要手动添加大量的边界检查和异常处理

4. **工具选择不当**：
   - 一开始 Cursor 生成代码时没有用 pydantic，导致结构化输出格式不稳定
   - 后来我根据项目需求（需要保证 JSON 格式严格符合要求）改用了 pydantic，这是基于我之前实习经验和对工具特性的理解

5. **和 Git / 构建系统配合时的小坑**：
   - **Git 冲突**：本地改了一些东西，它在旧版本基础上再改，提交/合并的时候冲突一堆
   - **对构建系统不敏感**：比如 Monorepo 下不同 package 的 build 脚本、特定 CI 规则，它经常"只改源码"，但没想到还有代码生成、编译参数、CI 校验等等
   - **本质问题**：它在文件层面很聪明，在工程化层面比较"半懂不懂"

6. **代码可读性和维护性**：
   - Cursor 一次性生成几百上千行代码，虽然功能能跑，但代码结构、命名风格不够统一
   - 我让 Cursor 生成后，再要求它写一份架构文档和 onboarding 文档，包括：项目结构、启动方式、关键模块说明
   - 这样既方便我自己理解代码，也方便团队协作和后续迭代

7. **测试覆盖不足**：
   - Cursor 生成的代码缺少单元测试，需要我手动补充测试用例
   - 特别是 PDF 解析这种复杂逻辑，需要针对不同格式的简历做大量测试，发现很多 edge case

8. **依赖管理和版本兼容**：
   - Cursor 有时会使用最新版本的库，但可能与项目其他依赖冲突
   - 需要手动调整依赖版本，确保环境一致性

总的来说，Cursor 适合快速搭建框架和生成样板代码，但在大型项目的全局理解、代码质量控制、工程化配合、细节优化、边界处理、测试验证这些方面，还是需要人工介入和迭代优化。

### 1.3 模型选择

**Q：你用的是哪个模型？**

**A：** 后端用的是 DeepSeek V3。因为 V3 支持 MOE 架构，不同任务需求能激活不同神经元，所以改简历和评价用同一个模型是 ok 的。之前在西门子实习也是用 DeepSeek 来判断和生成。

**Q：没测过别的模型吗？比如 OpenAI 的模型。**

**A：** 一开始想用性能强的模型，DeepSeek 3.2 感觉比较 ok，但 DeepSeek 调用硅基流动 API 比较慢，整个流程走完可能要 30 分钟。后来试了 Groq 平台的 OSS 120B 模型，几秒钟就能走完流程，但生成效果很差，简历写得很简单，所以还是保证效果优先，用 DeepSeek。

**Q：为什么没有用 GPT-4.5/5？**

**A：** 太贵了，我们只是想跑通流程，用 GPT-4.5 需要花美金，但用 DeepSeek 硅基流动新账户 20 块钱就能用。

**Q：你有没有给 LangChain 或者 LangGraph 做过贡献？**

**A：** 没有做贡献，但我对 LangChain/LangGraph 的使用感觉比较一般。因为它把很多东西都封装了，不够灵活。在西门子实习时，要用千问3的 embedding 模型，但 LangChain 集成的是 Elasticsearch，只能存最大 1024 维，千问3 embedding 有 4096 维，不支持。另外它不支持稀疏向量检索，但 BGE-M3 能输出稠密、稀疏、多向量三种模式。所以虽然工作流用 LangChain 做，但具体到每个节点，我都是用官方平台的 API 进行开发，而不是用 LangChain 套的模型。

### 1.4 Prompt 工程

**Q：你调了有多少版提示词？**

**A：** 主要就是修改和评价这两个提示词比较重要。我采用了系统化的 prompt 工程方法来优化：

**1. 结构化 Prompt 设计**：
   - **角色定义**：明确 agent 的角色（如"资深简历优化专家"），设定专业背景和职责边界
   - **任务分解**：将复杂任务拆分为清晰的步骤（解析 JD → 提取关键词 → 匹配简历 → 优化表达 → 格式检查）
   - **输出格式规范**：使用结构化输出（JSON Schema 或 pydantic），确保输出格式稳定可解析
   - **Few-shot 示例**：提供 2-3 个高质量的输入输出示例，让模型理解期望的效果

**2. 迭代优化流程**：
   - **初版生成**：修改的提示词一开始让 cursor 生成，效果很一般，没有对简历进行很大修改
   - **基准测试**：用 10-20 份不同类型的简历作为测试集，评估输出质量
   - **A/B 测试**：对比不同版本的 prompt（如是否加入 Few-shot、是否强调关键词匹配），量化评估指标（关键词覆盖率、格式正确率、用户满意度）
   - **版本管理**：用 Git 管理 prompt 版本，记录每次修改的原因和效果

**3. 评估指标体系**：
   - **定量指标**：关键词匹配率、格式符合度、输出长度合理性
   - **定性评估**：人工 review 样本，检查是否过度修改、是否保持原意
   - **边界测试**：测试极端情况（简历格式异常、JD 信息不全等）

**4. 最终优化**：
   - 后来朋友之前也用 AI 改简历，他把自己的 prompt 贡献出来，我们基于他的版本进一步优化
   - 结合我们的测试结果，调整了关键词提取的权重、优化了输出格式的约束
   - 评价的 agent prompt 相对简单，主要是打分标准和反馈格式，所以初版就基本满足需求，没有再大改

**经验总结**：
- Prompt 不是一次性写好的，需要基于实际效果迭代优化
- 建立评估体系很重要，不能只凭感觉判断好坏
- 结构化设计（角色、步骤、格式）比自由描述效果更好
- 版本管理帮助追踪哪些改动真正有效

**Q：你们做的 Hackathon 项目，其实它是个固定的流程对吧？第一步解析，第二步评价，第三步修改，然后生成总结，固定的流程对吧？**

**A：** 对，第一步是 Parser 解析，第二步是评价，第三步修改，然后同时去生成总结。固定的流程。

**Q：也没有说有个调度者说 ok，改完之后可能还要再评价一下，没有是吧？**

**A：** 我们有评价，就是会有招聘经理对改完之后的模型进行评价，如果打分超过 9 分就算通过，低于 9 分就打回去重新再改，一直循环。但没有说有一个总的 leader 来操控多个 agent 进行工作。

### 1.5 框架选择

**Q：你们当时为什么选择用 LangGraph 去写？**

**A：** 在选框架的时候，我其实对比过几类方案：

**1. 主流框架分类**：
   - **代码型框架**：、LangGraph、LlamaIndex
   - **低/无代码平台**：Dify、Flowise 等

**2. 各框架定位对比**：

   - **LangChain vs LangGraph 的主要区别**：
     
     **LangChain**：
     - **定位**：通用 LLM 应用框架，提供 models、tools、chains、agents 等一整套高层抽象
     - **特点**：组件化设计，适合快速搭建简单的 agent 或 chatbot，生态丰富、例子多
     - **控制流**：主要是链式调用（Sequential Chains）或简单的 Agent 循环，控制流相对简单
     - **状态管理**：在复杂、有状态的长流程里，控制流和状态管理要自己补很多东西
     - **适用场景**：快速原型、简单的问答系统、基础的 RAG 应用
     
     **LangGraph**：
     - **定位**：LangChain 团队出的有状态流程/Agent 编排框架，专门解决长流程、可恢复、可回溯的控制流问题
     - **特点**：基于图（Graph）的编排方式，把应用画成一张图（节点+边），然后它帮你管状态、重试、恢复等
     - **控制流**：支持复杂的条件分支、循环、并行执行、动态路由等
     - **状态管理**：内置状态持久化、检查点（checkpoint）机制，支持流程中断后恢复
     - **适用场景**：多步骤长流程、需要状态管理的复杂 Agent、生产环境部署
     
     **核心区别总结**：
     - LangChain = "组件库 + 简单链式调用"，适合快速搭建
     - LangGraph = "图编排 + 状态管理"，适合复杂流程
     - LangGraph 可以看作是 LangChain 在复杂流程编排上的"增强版"，两者可以配合使用（LangGraph 底层依赖 LangChain 的组件）

   - **LlamaIndex**：更偏数据/RAG 编排，强在索引私有数据、构建知识助手，对向量库、文档结构有很多高级封装。但做复杂控制流不是它的主战场。
   - **Dify/Flowise**：低代码平台，通过可视化方式搭工作流，适合快速 PoC 和团队内部使用。但复杂逻辑、版本控制、代码 review 会有点别扭，不太适合作为简历里展示工程能力的主角项目。

**3. 为什么最终选 LangGraph**：
   综合下来，我最终选了 LangGraph，主要有三点考虑：

   - **场景匹配度**：我的流程本质上是一张图：解析 JD → 解析简历 → 匹配打分 → 生成反馈，中间还有"异常分支"和"兜底策略"（比如打分低于 9 分要重新改）。用 LangGraph 可以非常自然地把这些节点和边建模成一个有状态的工作流。

   - **工程能力**：LangGraph 专门解决长流程、可恢复、可回溯的问题，具体来说：
     
     **长流程（Long-running Workflow）**：
     - 传统的一次性调用（如简单 API）是"请求-响应"模式，但我们的简历评估流程需要多个步骤：解析 JD → 解析简历 → 匹配打分 → 生成反馈 → 如果分数不够还要循环重改
     - 这种多步骤、可能持续几分钟甚至更久的流程，就是"长流程"
     - LangGraph 可以把整个流程建模成一个有状态的工作流，而不是简单的函数调用链
     
     **可恢复（Resumable）**：
     - 如果流程执行到一半（比如刚完成简历解析）时服务崩溃或网络中断，传统方式需要从头开始
     - LangGraph 支持状态持久化，可以把每个节点的执行状态（输入、输出、中间结果）保存到数据库
     - 服务恢复后可以从上次中断的节点继续执行，而不是重新开始，这对生产环境很重要
     
     **可回溯（Traceable/Replayable）**：
     - 可以完整记录整个流程的执行历史：哪个节点执行了、用了什么参数、输出了什么结果、花了多长时间
     - 这对于调试、A/B 测试、性能分析、问题排查都很有价值
     - 比如发现某个简历改得不好，可以回溯看是哪个 agent 在哪个步骤出了问题，然后针对性优化
     
     这些特性对后续做 A/B 测试或者线上化是有价值的。而且它是一个纯代码框架，既保持了 LangChain 生态的兼容性，又不会把架构锁死在某种黑盒 Agent 模式里，我可以自己设计 prompt 结构和打分逻辑，这更符合"做一个可维护、可拓展的工程项目"的目标。

   - **个人经验**：我之前在西门子实习的时候用过 LangGraph，上手成本比较低。

所以综合"场景匹配度 + 工程可维护性 + 个人经验"这几个因素，最后选择了用 LangGraph 来搭整个简历评估流程。

---

## 2. 实习经历相关

### 2.1 实习项目介绍

**Q：你给我讲讲你在实习的那个项目吧。**

**A：** 西门子的项目主要是做 Web 优化。我进去的时候，他们已经把整套系统做得能够问、能够把文档提取出来、把文档给到大语言模型回答，已经做完了。

**Q：一开始原先是哪个样子？**

**A：** 一开始是用 GLM 他们开发的一套框架 LangChain Chat，套那个模型、套那个框架来做。后来发现不太行，就用了 LangChain Chat 的 embedding 操作，剩下的前后端是自己开发的。当时他们没有评测的 pipeline。

**Q：他是静态的 RAG 还是动态的？**

**A：** 他就是一条链，就是检索检索完之后就把问题喂给模型，然后模型回答，没有说像我这种反复迭代、优化 query 的过程。他是到 Elasticsearch 检索，返回 TOP10 的数据，没有用到什么更高级的检索或者嵌入的方式。

**Q：也就说他只是用了向量检索，然后就 TOP10，是吧？**

**A：** 对，就是一个很基础的。

**Q：你改进了什么？**

**A：** 
- 第一个需求：测试不同小模型的性能，因为公司想部署小模型在本地上。我基于 LangSmith 写了一个测试的 pipeline，测试完之后效果挺一般，可能六七十的性能。
- 优化：5 月份千问3出来了，我开始修改 embedding 的问题。因为我发现主要错误还是文档提取不出来，测试的问题都是测试中心根据文档把问题提出来、把答案写出来，所以主要还是能不能把文档提取出来。
- 第一个优化：采用千问3的 embedding，但 LangChain Chat 集成的是 Elasticsearch，只能存最大 1024 维，千问3 embedding 有 4096 维，不支持。另外它不支持存储稀疏向量，但 BGE-M3 能输出三种模式（稠密、稀疏、多向量）。所以考虑换到 Milvus，它是比较专门的向量数据库，所有需求都满足。
- 重新改写 embedding 流程，放到 Milvus 数据库里面。
- 使用多路召回：除了用稠密（走千问的）、稀疏（走 BGE-M3 的），还有 BM25，因为发现关键词很重要。
- 召回完之后加一个重排序器 rank，将这些文档排好之后再喂给大模型。
- 最后准确率提到了 92%。

**Q：你们当时立的那个测试集大概有多少个问题？**

**A：** 测试 200 多个，180 左右是中文，90 多是英文。

**Q：你用了多路召回之后，你用的是哪个重排序？**

**A：** 当时考虑过多种方法：
- 一种是使用千问3他们家的 Relank 或者用 BGE-M3，基于大模型的 Relank。我们当时是自己本地部署的，测试一下性能，用千问3的模型，每 10 个文档会要加多 2 秒钟的运行时间。用 BGE-M3，30 个文档是 2 秒钟。
- 另外一种方法：leader 那边不太满意这个耗时，就用了 RF 的一个算法，根据每一个提取出来的文档的一个排名有个得分，根据得分再算一个总得分，然后最后把它重新重排序，得到一个文档效果。这样效率还挺高的，就是零点零几秒、几十毫秒的一个性能，就是一个排序的时间。但是准确率会有点点下降，因为没有走一遍大模型，可能没有办法判断哪些回答是真正跟问题相关的回答。

**Q：为什么你们采用了一个比较传统的 RAG 方法？**

**A：** 他们组本身不是做 AI 方面的工作，我们组本来是做 CTG 的软件开发，并没有专门做大模型的功能。后来是上面要求说要把这个东西做出来，然后就开始慢慢做。他们组也不做这个，组里面可能也是大家一起学习，就没有说专门有一个做算法或者做 agent 的人过来指导我们要怎么做，更多是大家一起探索学习，每周开会然后做。我作为一个实习生的话，我可以全力的做这个项目，但像他们正职的话，可能他们还有别的工作要求，并不像我就是专注于做这个项目。

### 2.2 项目优化方案

**Q：如果现在对你的项目的话，比如说你做一个外包，而且他们要给你付费，那你会怎样给他们做一个更好的 RAG 系统？**

**A：** 我会从架构、检索、生成、评估四个层面进行系统化改进：

**1. 架构层面：引入 Agent Loop 和自适应 Query Rewriting**

   - **动态 Query Rewriting**：
     - 虽然之前也做了 query 改写，但主要是针对中英文 gap 的静态改写
     - 现在会在 LangGraph 框架中加入一个 loop：先检索 → 判断检索结果是否足够回答问题 → 如果不够，动态改写 query（比如扩展同义词、添加专业术语、分解复杂问题）→ 再次检索
     - 这样可以解决"差一两个关键词找不到文档"的问题，提高召回率
   
   - **多轮检索策略**：
     - 如果第一轮检索结果不理想，可以尝试不同的检索策略（关键词检索、语义检索、混合检索）
     - 支持"检索-评估-再检索"的迭代过程，直到找到足够的相关文档

**2. 检索层面：Embedding 优化和 Chunk 策略**

   - **Embedding 模型微调**：
     - 当前问题：通用 embedding 模型对中英文语义理解不一致，对 CT 机专业术语效果不好
     - 解决方案：用现有的文档数据对 embedding 模型进行领域微调（Domain-specific Fine-tuning）
     - 可以显著提升专业术语和领域特定表达的检索准确率
   
   - **Chunk 策略优化**：
     - 当前是公司已经切好的 500 token 左右的 chunk，但可能不是最优的。分块策略直接影响检索效果，因为：
       - 如果 chunk 太大：可能包含多个不相关的主题，检索时噪声多
       - 如果 chunk 太小：可能丢失上下文信息，语义不完整
       - 如果按固定长度切：可能把一个完整的概念切断了
     
     可以尝试以下几种分块技术：
     
     **① 语义分块（Semantic Chunking）**：
     - **原理**：不是按固定长度切，而是根据语义边界（句子、段落、主题）来切分
     - **实现方式**：
       - 使用句子分割器（Sentence Splitter）识别句子边界
       - 计算相邻句子/段落的语义相似度（用 embedding 计算余弦相似度）
       - 当相似度低于阈值时，说明语义发生了转换，在这里切分
     - **优点**：保证每个 chunk 语义完整，不会把一个完整概念切断
     - **适用场景**：技术文档、长文章、需要保持语义连贯性的内容
     - **例子**：一段讲"CT 机操作步骤"的内容，不会在"第一步"和"第二步"中间切断
     
     **② 滑动窗口（Sliding Window / Overlapping Chunks）**：
     - **原理**：相邻的 chunk 之间有重叠部分，避免边界信息丢失
     - **实现方式**：
       - 固定 chunk 大小（如 500 tokens）
       - 设置重叠大小（如 50-100 tokens，通常是 chunk 大小的 10-20%）
       - 每次滑动时保留前一个 chunk 的末尾部分
     - **优点**：
       - 避免边界效应：如果一个概念刚好在 chunk 边界，不会完全丢失
       - 提高召回率：同一个概念可能出现在多个 chunk 中，增加被检索到的概率
     - **缺点**：会产生更多 chunk，存储和检索成本增加
     - **适用场景**：所有类型的文档，特别是需要高召回率的场景
     - **例子**：chunk1 是"步骤1-3"，chunk2 是"步骤3-5"（步骤3重叠），这样检索"步骤3"时两个 chunk 都可能被召回
     
     **③ 层次化分块（Hierarchical Chunking）**：
     - **原理**：根据文档的层次结构（标题、章节、段落）进行多粒度分块
     - **实现方式**：
       - **粗粒度**：按章节/大标题切分（如整个"操作指南"章节作为一个 chunk）
       - **中粒度**：按小节/小标题切分（如"开机步骤"作为一个 chunk）
       - **细粒度**：按段落/句子切分（如单个操作步骤）
       - 建立层次关系：粗粒度 chunk 包含多个中粒度 chunk，中粒度包含多个细粒度
     - **检索策略**：
       - 先检索细粒度 chunk（精确匹配）
       - 如果不够，再检索中粒度或粗粒度 chunk（提供更多上下文）
     - **优点**：
       - 既保证精确性（细粒度），又保证上下文完整性（粗粒度）
       - 可以根据问题复杂度选择不同粒度的 chunk
     - **适用场景**：结构化文档（技术手册、API 文档、操作指南）
     - **例子**：
       - 细粒度：单个"如何设置参数 X"的步骤
       - 中粒度：整个"参数设置"章节
       - 粗粒度：整个"系统配置"部分
     
     **④ 其他分块策略**：
     - **固定长度分块**：最简单，但可能切断语义（当前项目用的就是这种）
     - **递归分块**：先按大块切，如果太大再递归切小块，直到满足大小要求
     - **基于文档类型的分块**：
       - **技术文档**：按章节 + 语义分块
       - **FAQ**：每个问答对作为一个 chunk
       - **代码文档**：按函数/类分块
       - **操作手册**：按步骤分块，保持步骤完整性
     
     **选择建议**：
     - 对于 CT 机技术文档这种场景，推荐：**语义分块 + 滑动窗口 + 层次化**的组合
     - 先用层次化识别文档结构（章节、段落）
     - 在每层内部用语义分块保证语义完整
     - 用滑动窗口避免边界丢失
     - 这样既能保证语义完整，又能提高召回率
   
   - **Hybrid Search 增强**：
     - 当前已经有稠密向量、稀疏向量、BM25 的多路召回
     - 可以进一步优化：调整各路召回的权重、引入学习排序（Learning to Rank）、根据问题类型动态调整检索策略

**3. 生成层面：上下文压缩和 Summary 优化**

   - **上下文压缩（Context Compression）**：
     - 当前问题：检索到的文档可能很多很杂，模型 summary 能力有限，导致准确率下降
     - 解决方案：
       - 在喂给大模型前，先用一个"压缩 agent"对检索到的文档进行预处理，提取最相关的片段
       - 使用 **LongContextReorder** 等技术，优化文档在 prompt 中的排列顺序
       - 实现"摘要-扩展"机制：先给模型一个压缩版，如果不够再逐步扩展上下文
     
     **LongContextReorder 技术详解**：
     - **问题背景**：研究发现，大语言模型对 prompt 不同位置的关注度不同，存在"位置偏差"（Position Bias）：
       - **开头位置**：模型会仔细阅读，但可能被后面的内容"稀释"注意力
       - **中间位置**：模型关注度最高，信息利用率最好
       - **结尾位置**：模型也会关注，但可能因为前面内容太多而注意力分散
     
     - **原理**：
       - 传统方式：按检索相关性排序，最相关的文档放在最前面
       - LongContextReorder：重新排列文档顺序，把最相关的文档放在 prompt 的中间位置
       - 具体实现：
         1. 先按相关性排序所有检索到的文档
         2. 计算每个文档与 query 的相关性分数
         3. 重新排列：把最相关的文档放在中间，次相关的放在开头和结尾
         4. 形成"U 型"或"倒 U 型"的排列模式
     
     - **效果**：
       - 提高模型对关键信息的利用率
       - 在长上下文场景下（如检索到 10+ 个文档），效果提升更明显
       - 通常可以提升 5-15% 的答案准确率
     
     - **适用场景**：
       - 检索到大量文档（>5 个）时效果明显
       - 长上下文模型（如 GPT-4、Claude 等支持长 context 的模型）
       - 需要从多个文档中综合信息的复杂问题
     
     - **其他上下文优化技术**：
       - **Contextual Compression**：用另一个 LLM 压缩每个文档，只保留与 query 相关的部分
       - **Summary Compression**：对每个文档生成摘要，用摘要替代完整文档
       - **Relevance Filtering**：设置相关性阈值，过滤掉低相关性的文档
       - **Progressive Summarization**：先给模型压缩版，如果回答不够好，再逐步提供更多细节
   
   - **分阶段生成**：
     - 对于复杂问题，可以拆分为多个子问题，分别检索和生成，最后再整合
     - 使用 **Chain of Thought (CoT)** 或 **Tree of Thoughts (ToT)** 让模型更好地处理多文档信息
     
     **Chain of Thought (CoT) 思维链技术**：
     - **原理**：
       - 传统方式：模型直接给出最终答案，中间推理过程不透明
       - CoT：要求模型展示推理过程，一步一步思考，最后得出结论
       - 核心思想："Let's think step by step"（让我们一步步思考）
     
     - **在 RAG 中的应用**：
       - **步骤 1**：理解用户问题，识别需要哪些信息
       - **步骤 2**：分析检索到的文档，提取相关信息
       - **步骤 3**：整合多个文档的信息，识别一致性和冲突
       - **步骤 4**：基于整合的信息，推理出答案
       - **步骤 5**：验证答案的完整性和准确性
     
     - **Prompt 示例**：
       ```
       用户问题：CT 机如何设置参数 X？
       
       检索到的文档：
       - 文档1：参数 X 的基本设置方法
       - 文档2：参数 X 的注意事项
       - 文档3：参数 X 与其他参数的关系
       
       请按以下步骤思考：
       1. 首先，从文档1中提取参数 X 的设置步骤
       2. 然后，检查文档2中是否有需要注意的限制条件
       3. 接着，确认文档3中参数 X 与其他参数的依赖关系
       4. 最后，综合以上信息，给出完整的设置指南
       ```
     
     - **优点**：
       - 提高推理准确性：让模型"慢思考"，减少错误
       - 可解释性：可以看到模型的推理过程，便于调试
       - 适合复杂问题：多步骤、需要综合多个文档的问题
     
     - **适用场景**：
       - 需要综合多个文档信息的复杂问题
       - 需要多步骤推理的问题（如"如何操作"、"为什么"）
       - 需要验证答案准确性的场景
     
     **Tree of Thoughts (ToT) 思维树技术**：
     - **原理**：
       - CoT 是线性的：A → B → C → 答案
       - ToT 是树状的：从问题出发，探索多个可能的推理路径，然后评估每条路径，选择最优的
       - 核心思想：不是只有一条推理路径，而是探索多条路径，选择最好的
     
     - **在 RAG 中的应用**：
       - **步骤 1**：问题分解 - 将复杂问题拆分为多个子问题
       - **步骤 2**：多路径探索 - 对每个子问题，尝试不同的检索策略和推理路径
         - 路径 A：直接检索相关文档 → 提取信息 → 生成答案
         - 路径 B：先改写 query → 检索 → 提取信息 → 生成答案
         - 路径 C：分阶段检索（先检索概览，再检索细节）→ 整合 → 生成答案
       - **步骤 3**：路径评估 - 评估每条路径的中间结果质量
       - **步骤 4**：路径选择 - 选择最优路径继续推理
       - **步骤 5**：答案整合 - 综合多条路径的信息，生成最终答案
     
     - **Prompt 示例**：
       ```
       用户问题：为什么 CT 机在参数 X 设置后会出现故障 Y？
       
       让我们探索不同的推理路径：
       
       路径 1：直接检索
       - 检索"参数 X 和故障 Y 的关系"
       - 结果：找到相关文档
       
       路径 2：分步检索
       - 步骤1：检索"参数 X 的作用"
       - 步骤2：检索"故障 Y 的原因"
       - 步骤3：检索两者的关联
       - 结果：找到更全面的信息
       
       路径 3：因果链推理
       - 步骤1：检索"参数 X 的设置方法"
       - 步骤2：检索"参数 X 设置不当的后果"
       - 步骤3：检索"故障 Y 的触发条件"
       - 结果：建立了完整的因果链
       
       评估每条路径的信息完整性和相关性，选择最优路径生成答案。
       ```
     
     - **优点**：
       - 探索性更强：不局限于单一推理路径
       - 容错性更好：如果一条路径失败，可以尝试其他路径
       - 答案质量更高：通过多路径探索和评估，找到最优答案
     
     - **缺点**：
       - 计算成本高：需要多次检索和推理
       - 实现复杂：需要设计评估和选择机制
     
     - **适用场景**：
       - 非常复杂的问题，需要多角度分析
       - 不确定性问题，需要探索多种可能性
       - 对答案质量要求极高的场景
     
     **CoT vs ToT 选择建议**：
     - **简单到中等复杂度问题**：用 CoT，成本低、效果好
     - **非常复杂的问题**：用 ToT，虽然成本高，但答案质量更好
     - **实际应用**：可以先尝试 CoT，如果效果不够好，再升级到 ToT
     
     **具体实现方式**：
     
     **① CoT 在 LangGraph 中的实现**：
     
     **方法 1：多节点链式实现**
     ```python
     from langgraph.graph import StateGraph, END
     from typing import TypedDict
     
     class CoTState(TypedDict):
         query: str
         retrieved_docs: list
         step1_analysis: str  # 步骤1：理解问题
         step2_extraction: str  # 步骤2：提取信息
         step3_integration: str  # 步骤3：整合信息
         step4_reasoning: str  # 步骤4：推理答案
         step5_verification: str  # 步骤5：验证
         final_answer: str
     
     def step1_understand_query(state: CoTState):
         """步骤1：理解用户问题，识别需要哪些信息"""
         prompt = f"""
         用户问题：{state['query']}
         
         请分析这个问题需要哪些信息：
         1. 问题的核心是什么？
         2. 需要检索哪些类型的文档？
         3. 需要提取哪些关键信息？
         
         请逐步思考并输出分析结果。
         """
         result = llm.invoke(prompt)
         return {"step1_analysis": result}
     
     def step2_extract_info(state: CoTState):
         """步骤2：分析检索到的文档，提取相关信息"""
         prompt = f"""
         用户问题：{state['query']}
         检索到的文档：{state['retrieved_docs']}
         上一步分析：{state['step1_analysis']}
         
         请按以下步骤提取信息：
         1. 从每个文档中提取与问题相关的关键信息
         2. 标记信息的来源（哪个文档）
         3. 评估信息的相关性和可信度
         
         请逐步思考并输出提取结果。
         """
         result = llm.invoke(prompt)
         return {"step2_extraction": result}
     
     def step3_integrate_info(state: CoTState):
         """步骤3：整合多个文档的信息"""
         prompt = f"""
         用户问题：{state['query']}
         提取的信息：{state['step2_extraction']}
         
         请整合信息：
         1. 识别信息之间的一致性
         2. 发现信息之间的冲突
         3. 解决冲突（优先使用更可信的来源）
         4. 形成完整的信息视图
         
         请逐步思考并输出整合结果。
         """
         result = llm.invoke(prompt)
         return {"step3_integration": result}
     
     def step4_reason_answer(state: CoTState):
         """步骤4：基于整合的信息，推理出答案"""
         prompt = f"""
         用户问题：{state['query']}
         整合的信息：{state['step3_integration']}
         
         请基于整合的信息推理答案：
         1. 回顾问题的核心
         2. 基于整合的信息进行推理
         3. 得出结论
         
         请逐步思考并输出推理过程和答案。
         """
         result = llm.invoke(prompt)
         return {"step4_reasoning": result}
     
     def step5_verify_answer(state: CoTState):
         """步骤5：验证答案的完整性和准确性"""
         prompt = f"""
         用户问题：{state['query']}
         推理的答案：{state['step4_reasoning']}
         原始文档：{state['retrieved_docs']}
         
         请验证答案：
         1. 检查答案是否完整回答了问题
         2. 验证答案是否与原始文档一致
         3. 检查是否有遗漏的重要信息
         4. 如果发现问题，修正答案
         
         请逐步思考并输出最终答案。
         """
         result = llm.invoke(prompt)
         return {"step5_verification": result, "final_answer": result}
     
     # 构建 LangGraph
     workflow = StateGraph(CoTState)
     workflow.add_node("understand", step1_understand_query)
     workflow.add_node("extract", step2_extract_info)
     workflow.add_node("integrate", step3_integrate_info)
     workflow.add_node("reason", step4_reason_answer)
     workflow.add_node("verify", step5_verify_answer)
     
     workflow.set_entry_point("understand")
     workflow.add_edge("understand", "extract")
     workflow.add_edge("extract", "integrate")
     workflow.add_edge("integrate", "reason")
     workflow.add_edge("reason", "verify")
     workflow.add_edge("verify", END)
     
     app = workflow.compile()
     ```
     
     **方法 2：单节点 Prompt 实现（更简单）**
     ```python
     def cot_rag_agent(state: CoTState):
         """在一个节点中实现 CoT，通过精心设计的 prompt"""
         prompt = f"""
         用户问题：{state['query']}
         检索到的文档：{state['retrieved_docs']}
         
         请按照以下步骤逐步思考并回答问题：
         
         【步骤 1：理解问题】
         分析这个问题的核心是什么？需要哪些信息？
         思考：...
         
         【步骤 2：提取信息】
         从检索到的文档中提取相关信息：
         - 文档1：...
         - 文档2：...
         思考：...
         
         【步骤 3：整合信息】
         整合多个文档的信息，识别一致性和冲突：
         - 一致性：...
         - 冲突：...
         思考：...
         
         【步骤 4：推理答案】
         基于整合的信息，推理出答案：
         推理过程：...
         结论：...
         
         【步骤 5：验证答案】
         验证答案的完整性和准确性：
         - 是否完整回答了问题：...
         - 是否与文档一致：...
         最终答案：...
         """
         result = llm.invoke(prompt)
         return {"final_answer": result}
     ```
     
     **② ToT 在 LangGraph 中的实现**：
     
     ```python
     from langgraph.graph import StateGraph, END
     from typing import TypedDict, List
     import asyncio
     
     class ToTState(TypedDict):
         query: str
         paths: List[dict]  # 多条推理路径
         path_scores: dict  # 每条路径的评分
         best_path: dict  # 最优路径
         final_answer: str
     
     def decompose_query(state: ToTState):
         """步骤1：问题分解"""
         prompt = f"""
         用户问题：{state['query']}
         
         请将这个问题分解为多个子问题，以便从不同角度分析。
         子问题1：...
         子问题2：...
         子问题3：...
         """
         result = llm.invoke(prompt)
         return {"sub_queries": parse_sub_queries(result)}
     
     def explore_path_a(state: ToTState):
         """路径 A：直接检索"""
         query = state['query']
         docs = retrieve_documents(query)  # 直接检索
         answer = generate_answer(query, docs)
         return {
             "paths": state.get("paths", []) + [{
                 "path_id": "A",
                 "strategy": "直接检索",
                 "docs": docs,
                 "answer": answer
             }]
         }
     
     def explore_path_b(state: ToTState):
         """路径 B：改写 query 后检索"""
         query = state['query']
         rewritten_query = rewrite_query(query)  # 改写 query
         docs = retrieve_documents(rewritten_query)
         answer = generate_answer(query, docs)
         return {
             "paths": state.get("paths", []) + [{
                 "path_id": "B",
                 "strategy": "改写query后检索",
                 "rewritten_query": rewritten_query,
                 "docs": docs,
                 "answer": answer
             }]
         }
     
     def explore_path_c(state: ToTState):
         """路径 C：分阶段检索"""
         query = state['query']
         # 阶段1：检索概览
         overview_docs = retrieve_documents(f"{query} 概览")
         # 阶段2：检索细节
         detail_docs = retrieve_documents(f"{query} 详细步骤")
         all_docs = overview_docs + detail_docs
         answer = generate_answer(query, all_docs)
         return {
             "paths": state.get("paths", []) + [{
                 "path_id": "C",
                 "strategy": "分阶段检索",
                 "docs": all_docs,
                 "answer": answer
             }]
         }
     
     def evaluate_paths(state: ToTState):
         """步骤3：评估每条路径"""
         paths = state['paths']
         scores = {}
         
         for path in paths:
             # 评估维度：信息完整性、相关性、答案质量
             evaluation_prompt = f"""
             路径策略：{path['strategy']}
             检索到的文档数量：{len(path['docs'])}
             生成的答案：{path['answer']}
             原始问题：{state['query']}
             
             请评估这条路径：
             1. 信息完整性（0-10分）：文档是否覆盖了问题的所有方面
             2. 相关性（0-10分）：文档与问题的相关程度
             3. 答案质量（0-10分）：答案的准确性和完整性
             
             输出格式：完整性:X, 相关性:Y, 答案质量:Z, 总分:W
             """
             result = llm.invoke(evaluation_prompt)
             score = parse_score(result)
             scores[path['path_id']] = score
         
         # 选择最优路径
         best_path_id = max(scores, key=scores.get)
         best_path = next(p for p in paths if p['path_id'] == best_path_id)
         
         return {
             "path_scores": scores,
             "best_path": best_path
         }
     
     def generate_final_answer(state: ToTState):
         """步骤4：基于最优路径生成最终答案"""
         best_path = state['best_path']
         all_paths = state['paths']
         
         # 可以综合多条路径的信息
         prompt = f"""
         用户问题：{state['query']}
         最优路径：{best_path['strategy']}
         最优路径答案：{best_path['answer']}
         其他路径信息：{[p['answer'] for p in all_paths if p['path_id'] != best_path['path_id']]}
         
         请综合所有路径的信息，生成最终答案。
         如果其他路径有补充信息，可以整合进来。
         """
         final_answer = llm.invoke(prompt)
         return {"final_answer": final_answer}
     
     # 构建 LangGraph（支持并行探索）
     workflow = StateGraph(ToTState)
     workflow.add_node("decompose", decompose_query)
     workflow.add_node("path_a", explore_path_a)
     workflow.add_node("path_b", explore_path_b)
     workflow.add_node("path_c", explore_path_c)
     workflow.add_node("evaluate", evaluate_paths)
     workflow.add_node("finalize", generate_final_answer)
     
     workflow.set_entry_point("decompose")
     workflow.add_edge("decompose", "path_a")
     workflow.add_edge("decompose", "path_b")
     workflow.add_edge("decompose", "path_c")
     # 等待所有路径完成后再评估
     workflow.add_edge("path_a", "evaluate")
     workflow.add_edge("path_b", "evaluate")
     workflow.add_edge("path_c", "evaluate")
     workflow.add_edge("evaluate", "finalize")
     workflow.add_edge("finalize", END)
     
     app = workflow.compile()
     ```
     
     **实现要点总结**：
     
     **CoT 实现要点**：
     1. **多节点方式**：每个推理步骤作为一个节点，清晰可控，但节点多、流程长
     2. **单节点方式**：在一个 prompt 中完成所有步骤，简单快速，但控制力较弱
     3. **关键**：Prompt 中明确要求"逐步思考"，让模型展示推理过程
     
     **ToT 实现要点**：
     1. **并行探索**：多条路径可以并行执行（如用 asyncio），提高效率
     2. **评估机制**：需要设计评估函数，可以从多个维度评分（信息完整性、相关性、答案质量）
     3. **路径选择**：根据评分选择最优路径，也可以综合多条路径的信息
     4. **成本控制**：可以限制探索的路径数量，或者设置评估阈值，提前终止低质量路径
     
     **实际应用建议**：
     - **简单场景**：用 CoT 的单节点方式，快速实现
     - **复杂场景**：用 CoT 的多节点方式，更好的控制和可观测性
     - **非常复杂场景**：用 ToT，但要注意成本控制，可以限制路径数量或设置评估阈值

**4. 评估体系：客观指标和持续优化**

   - **引入召回率评估**：
     - 当前问题：只用大模型评估大模型，有波动且不够客观
     - 解决方案：
       - 建立标准测试集，每个问题标注"标准答案文档"（Ground Truth Documents）
       - 计算召回率（Retrieval Recall）：检索到的文档中有多少是标准答案文档
       - 这是更客观的评估指标，因为"能否找到文档"是 RAG 系统的核心能力
   
   - **多维度评估**：
     - 检索阶段：召回率（Recall）、精确率（Precision）、MRR（Mean Reciprocal Rank）
     - 生成阶段：答案准确性、相关性、完整性
     - 端到端：用户满意度、任务完成率
   
   - **A/B 测试框架**：
     - 建立实验框架，可以对比不同检索策略、不同 embedding 模型、不同生成策略的效果
     - 支持在线 A/B 测试，根据实际使用数据持续优化

**5. 工程化改进：可观测性和监控**

   - **可观测性（Observability）**：
     - 记录每个环节的指标：检索耗时、检索文档数、生成耗时、token 消耗
     - 建立监控 dashboard，实时查看系统性能
   
   - **错误分析和反馈循环**：
     - 记录失败案例，分析是检索问题还是生成问题
     - 建立用户反馈机制，持续收集 bad cases 并优化

**总结**：
   核心思路是"从静态 RAG 升级到动态、自适应的 RAG 系统"，通过 Agent Loop、Query Rewriting、Embedding 微调、上下文压缩等技术，提升系统的召回率和准确率。同时建立完善的评估体系和监控机制，确保系统可以持续优化。

**Q：你们当时整个问答的对象文档有多少个？**

**A：** 陆续往里面添加。我们是一个机型有大概 5000 个文档，然后后续会往里面添加。因为主要是一些标准的问题，可能说有些文档我们是不能给客户看到的，然后后面可能说我们又能开放出来这些文档给客户看到，这个时候我们要再往里面再加。或者说加入新的机型的时候，我们就要加新的文档。

**Q：有没有按照机型分个类？**

**A：** 有的，按照机型分类的。不同的机型，我只能提取到相对应机型的文档，不能说把机型的文档串了，因为会有幻觉什么的。源数据它只能搜索相应的一些文档。

**Q：他是在哪一步完成的？**

**A：** 检索的时候就是我嵌入的时候是能够加这个元数据的，然后我检索的时候就根据这个元数据来进行一个筛选 filter，把这些无关的数据全部 filter 掉。

**Q：chunk 的话是怎样去切分的？**

**A：** 我们当时是没有切 chunk，就是因为他们给到我们的时候就是已经切好了，就是一小段一小段，就是按照公司的那种想法已经切好了，所以说我们就不方便说再进行再切了。他们有可能是一个一整个小章节切分，也可能是一个章节的一个小点切分，一个章节一个小点切分，但他们给到我的时候就是已经是这样的。他们文档也不算特别大，当时看的是一个文档大概就 500 个 TOKEN 左右，所以说我觉得已经没有必要再切了，就没有考虑过这件事。

---

